# ğŸ“– Regression fine-tuning

This project automates the fine-tuning of sentence embeddings for regression tasks and evaluates them using supervised learning (XGBoost Random Forest). 

The idea behind this is to make embeddings context-aware in terms of price or another numerical attribute.

We use an [open-source dataset](https://www.kaggle.com/datasets/promptcloud/amazon-product-dataset-2020) with Amazon product descriptions and price.

Since our target is numerical, we normalize the target variable (asin_value_usd) to improve model performance and stability.

* Cap Outliers â†’ `RobustScaler` to reduce the impact of extreme values.
* Normalize the Range â†’ `MinMaxScaler` to scale prices between (-1.0, 1.0).

## Overview

1ï¸âƒ£ `run_fine_tuning_experiments.py`

- Fine-tunes a MiniLM sentence transformer on pricing data.
- Supports multiple fine-tuning strategies (e.g., last N layers, full model, classification head).
- Saves fine-tuned models for later use.

2ï¸âƒ£ `run_supervised_evaluation.py`

- Loads the fine-tuned models.
- Extracts embeddings from the trained models.
- Evaluates performance using XGBoost RF regression on extracted embeddings.

3ï¸âƒ£ `run_full_pipeline.py`

- Master script that runs both fine-tuning and evaluation sequentially.
- Ensures that fine-tuning is completed before evaluation starts.
- Displays live output logs using `rich` library.

## Results

After training and evaluation, the pipeline displays:

âœ… Validation RMSE & MAE (during training)  
âœ… Supervised Regression RMSE & MAE (on extracted embeddings)  

ğŸ“Š Final results are presented in a Rich table.

```plain
ğŸ“‚ regression-fine-tuning/
â”‚â”€â”€ ğŸ“œ run_fine_tuning_experiments.py
â”‚â”€â”€ ğŸ“œ run_supervised_evaluation.py
â”‚â”€â”€ ğŸ“œ run_full_pipeline.py
â”‚â”€â”€ ğŸ“œ requirements.txt
â”‚â”€â”€ ğŸ“œ README.md
â”‚â”€â”€ ğŸ“‚ models/
```