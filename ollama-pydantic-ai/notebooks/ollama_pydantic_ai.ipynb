{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Pydantic AI with Deepseek R1\n",
    "\n",
    "A small prototype for Pydantic AI agentic workflow.\n",
    "\n",
    "Ollama client needs to be installed to access the local LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple local call to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Connect to local Ollama instance\n",
    "client = openai.Client(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-r1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is Pydantic AI?\"}],\n",
    "    temperature=0.0,\n",
    "    stop=[\"<think></think>\"],\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Response:**\n",
       "\n",
       "<think>\n",
       "\n",
       "</think>\n",
       "\n",
       "Pydantic AI refers to the use of Pydantic, a Python library for data validation and serialization, in conjunction with artificial intelligence (AI) techniques. This combination allows for enhanced data handling, validation, and transformation in AI applications.\n",
       "\n",
       "### Key Components:\n",
       "1. **Pydantic**: A lightweight library that makes data validation easier by providing tools to define, validate, and serialize data structures.\n",
       "2. **AI Techniques**: Machine learning models or algorithms that can be applied alongside Pydantic's validation capabilities.\n",
       "\n",
       "### Applications of Pydantic AI:\n",
       "1. **Data Cleaning**: AI-driven methods can automate the detection and correction of invalid or inconsistent data using Pydantic's validation rules.\n",
       "2. **Schema Learning**: Pydantic can work with AI models to learn data schemas from datasets, enabling automatic validation without explicit schema definitions.\n",
       "3. **Error Analysis**: By integrating AI, Pydantic can analyze why certain data entries are invalid and provide insights for improving data quality.\n",
       "4. **Transformers**: AI-based transformers can preprocess or postprocess data using Pydantic's structure to ensure compliance with desired formats.\n",
       "\n",
       "### Benefits:\n",
       "- **Automation**: Reduces manual effort in data validation.\n",
       "- **Scalability**: Handles large datasets efficiently.\n",
       "- **Flexibility**: Combines the strengths of both Python and AI for versatile solutions.\n",
       "\n",
       "### Use Cases:\n",
       "- **Financial Services**: Automating validation of financial data to prevent errors or fraud.\n",
       "- **Healthcare**: Ensuring patient data adheres to strict standards before analysis.\n",
       "- **E-commerce**: Validating user inputs like orders, ratings, and reviews.\n",
       "\n",
       "In summary, Pydantic AI leverages the power of Python's Pydantic library with AI techniques to create robust solutions for data validation, transformation, and automation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def pretty_print_response(response):\n",
    "    \"\"\"\n",
    "    Displays the LLM response in a clean, readable format using Markdown.\n",
    "    \"\"\"\n",
    "    display(Markdown(f\"**Response:**\\n\\n{response}\"))\n",
    "\n",
    "pretty_print_response(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Pydantic AI workflow\n",
    "\n",
    "[**ollama_example.py**](https://ai.pydantic.dev/models/#example-local-usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: A Python decorator using a class with a '__call__' method\n",
      "\n",
      "Python Code:\n",
      " @property\n",
      "\n",
      "class Deco:\n",
      "  def __init__(self, func):\n",
      "    self.func = func\n",
      "\n",
      "  def __call__(self):\n",
      "    return self(func)\n",
      "\n",
      "def deco(func):return Deco(func)\n",
      "\n",
      "Usage stats: Usage(requests=1, request_tokens=196, response_tokens=84, total_tokens=280, details=None)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define your structured output\n",
    "class Explanation(BaseModel):\n",
    "    summary: str\n",
    "    python_function: str\n",
    "\n",
    "# Set up the Ollama-compatible model\n",
    "ollama_model = OpenAIModel(\n",
    "    model_name=\"llama3.1:8b\",\n",
    "    provider=OpenAIProvider(base_url=\"http://localhost:11434/v1\"),\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "# Create the agent that will handle structured responses\n",
    "agent = Agent(model=ollama_model, result_type=Explanation)\n",
    "\n",
    "# Run the agent with a natural language query\n",
    "result = agent.run_sync(\n",
    "    \"\"\"\n",
    "    Write a Python decorator using a class with a '__call__' method.\n",
    "    It must not use closures or nested functions.\n",
    "    \"\"\"\n",
    ")\n",
    "# Access parsed response\n",
    "print(\"Summary:\", result.data.summary)\n",
    "print(\"\\nPython Code:\\n\", result.data.python_function)\n",
    "\n",
    "# Optional: Token usage\n",
    "print(\"\\nUsage stats:\", result.usage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex Pydantic AI workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Subtasks:\n",
      "- : Load data from S3\n",
      "- : Clean the data\n",
      "- : Apply PCA to the cleaned data\n",
      "- : Save the transformed data back to S3\n",
      "\n",
      "ðŸ§ª Generated Code Snippets:\n",
      "\n",
      "ðŸ”§ Load data from S3:\n",
      "\n",
      "def load_data_from_s3(bucket_name, file_key):    s3 = boto3.client('s3')    try:        obj = s3.get_object(Bucket=bucket_name, Key=file_key)        data = obj['Body'].read()        return data    except Exception as e:        print(f'Error loading S3 object: {e}')        return None\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ”§ Cleaned Data Function:\n",
      "\n",
      "import pandas as pd\n",
      "def clean_data(df):\n",
      "    # drop missing values\n",
      "    df = df.dropna()\n",
      "    # remove duplicates\n",
      "    df = df.drop_duplicates()\n",
      "    return df\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ”§ Plain text responses are not permitted, please call one of the functions instead.:\n",
      "\n",
      "print('Function called successfully.')\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ”§ save_transformed_data_to_s3:\n",
      "\n",
      "import boto3\n",
      "bucket_name = 'my-bucket'\n",
      "s3_client = boto3.client('s3')\n",
      "def load_transformed_data(bucket):\n",
      "    response = s3_client.list_objects(Bucket=bucket)\n",
      "    transformed_data = response['Contents'][0]['Key']\n",
      "    return transformed_data\n",
      "transformed_data = load_transformed_data(bucket_name)\n",
      "s3_client.put_object(Body= transformed_data, Bucket= bucket_name, Key='transformed_data.csv')\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from retry import retry\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Schema definition\n",
    "class Subtask(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "class SubtasksList(BaseModel):\n",
    "    tasks: List[Subtask]\n",
    "\n",
    "\n",
    "class GeneratedFunction(BaseModel):\n",
    "    task_name: str\n",
    "    python_code: str\n",
    "\n",
    "\n",
    "# Ollama-compatible model\n",
    "ollama_model = OpenAIModel(\n",
    "    model_name=\"llama3.1:8b\",\n",
    "    provider=OpenAIProvider(base_url=\"http://localhost:11434/v1\"),\n",
    "    api_key=\"ollama\",\n",
    ")\n",
    "\n",
    "# Agent using the structured schema\n",
    "extract_agent = Agent(model=ollama_model, result_type=SubtasksList)\n",
    "\n",
    "\n",
    "# Retry wrapper\n",
    "# @retry(tries=5, delay=2, backoff=2)\n",
    "def extract_subtasks(description: str):\n",
    "    return extract_agent.run_sync(\n",
    "        f\"\"\"\n",
    "        You are a task extraction assistant.\n",
    "\n",
    "        Please respond with JSON structured as:\n",
    "        {{\n",
    "        \"tasks\": [\n",
    "            {{\"name\": \"...\", \"description\": \"...\"}},\n",
    "            ...\n",
    "        ]\n",
    "        }}\n",
    "\n",
    "        Here is the description to analyze: {description}\n",
    "        \"\"\",\n",
    "        model_settings={\"max_retries\": 5},\n",
    "    )\n",
    "\n",
    "\n",
    "# Description of the task for Agent\n",
    "description = \"\"\"\n",
    "Build a pipeline that loads data from S3, cleans it, applies PCA, and saves it back.\n",
    "\"\"\"\n",
    "\n",
    "# Print tasks to execute\n",
    "try:\n",
    "    result = extract_subtasks(description)\n",
    "    print(\"\\nExtracted Subtasks:\")\n",
    "    for task in result.data.tasks:\n",
    "        print(f\"- {task.name}: {task.description}\")\n",
    "except Exception as e:\n",
    "    print(\"\\nStill failed after retries:\", e)\n",
    "\n",
    "# New agent to generate code per task\n",
    "codegen_agent = Agent(model=ollama_model, result_type=GeneratedFunction)\n",
    "\n",
    "print(\"\\nðŸ§ª Generated Code Snippets:\\n\")\n",
    "for task in result.data.tasks:\n",
    "    codegen_prompt = (\n",
    "        f\"Write a Python function for the task '{task.name}': {task.description}. \"\n",
    "        \"The function should be realistic and self-contained.\"\n",
    "    )\n",
    "    code_result = codegen_agent.run_sync(codegen_prompt)\n",
    "    print(f\"ðŸ”§ {code_result.data.task_name}:\\n\")\n",
    "    print(code_result.data.python_code)\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
