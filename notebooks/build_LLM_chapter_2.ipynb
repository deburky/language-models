{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Build a Large Language Model (from scratch)\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\">\n",
    "\n",
    "Author of notes: https://github.com/deburky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Working with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM tokenizers\n",
    "\n",
    "`SimpleTokenizerV1`\n",
    "\n",
    "When we want to convert the outputs of an LLM from numbers back into text, we need a way to turn token IDs into text. For this, we can create an inverse version of the vocabulary that maps token IDs back to the corresponding text tokens.\n",
    "\n",
    "Let's implement a complete tokenizer class in Python with an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary. In addition, we'll implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text. The following listing shows the code for this tokenizer implementation.\n",
    "\n",
    "`SimpleTokenizerV2`\n",
    "\n",
    "We need to modify the tokenizer to handle unknown words. We also need to address the usage and addition of special context tokens that can enhance a model‚Äôs understanding of context or other relevant information in the text. These special tokens can include markers for unknown words and document boundaries, for example. In particular, we will modify the vocabulary and tokenizer, `SimpleTokenizerV2`, to support two new tokens, `<|unk|>` and `<|endoftext|>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "Total number of tokens: 4649\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Converting tokens into token IDs;\n",
       "    This conversion is an intermediate step before\n",
       "    converting the token IDs into embedding vectors."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1159\n",
      "Encoding: [1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n",
      "Decoding: \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
      "\n",
      "\n",
      "Vocabulary size after extension: 1161\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "As an additional quick check, let's print the last\n",
       "    five entries of the updated vocabulary:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r\"\\1\", text)\n",
    "        return text\n",
    "\n",
    "\n",
    "# Path to text data\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "    \"the-verdict.txt\"\n",
    ")\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "# Reading the training data\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(f\"Total number of character: {len(raw_text)}\")\n",
    "# print(raw_text[:99])\n",
    "\n",
    "# Converting the entire text in a training dataset into tokens\n",
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(f\"Total number of tokens: {len(preprocessed)}\")\n",
    "# print(preprocessed[:30])\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"Converting tokens into token IDs;\n",
    "    This conversion is an intermediate step before\n",
    "    converting the token IDs into embedding vectors.\"\"\"\n",
    "))\n",
    "\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    # print(item)\n",
    "    if i >= 50:\n",
    "        break\n",
    "\n",
    "simple_tokenizer_v1 = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = simple_tokenizer_v1.encode(text)\n",
    "\n",
    "print(f\"Encoding: {ids}\")\n",
    "print(f\"Decoding: {simple_tokenizer_v1.decode(ids)}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "# text = \"do you know what is the meaning of life?\"\n",
    "# print(f\"Unseen text: {simple_tokenizer_v1.encode(text)}\")\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "print(f\"Vocabulary size after extension: {len(vocab.items())}\")\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"As an additional quick check, let's print the last\n",
    "    five entries of the updated vocabulary:\"\"\"\n",
    "))\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Based on the output of this print statement, the new vocabulary size is 1,161 (the previous vocabulary size was 1,159)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "Encoding: [1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n",
      "Decoding: <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r\"\\1\", text)\n",
    "        return text\n",
    "\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "# 2nd version of tokenizer\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(f\"Encoding: {tokenizer.encode(text)}\")\n",
    "print(f\"Decoding: {tokenizer.decode(tokenizer.encode(text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîñ **Special tokens**:\n",
    "\n",
    "- **[BOS]** (beginning of sequence)‚Äâ‚ÄîThis token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
    "- **[EOS]** (end of sequence)‚Äâ‚ÄîThis token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one ends and the next begins.\n",
    "- **[PAD]** (padding)‚Äâ‚ÄîWhen training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or ‚Äúpadded‚Äù using the [PAD] token, up to the length of the longest text in the batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte pair econding (BPE)\n",
    "\n",
    "If the tokenizer encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">tiktoken version: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "tiktoken version: \u001b[1;36m0.7\u001b[0m.\u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Integers: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15496</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">466</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">345</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">588</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8887</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">220</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">554</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4252</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18250</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8812</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2114</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">286</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">617</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34680</span>, \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27271</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Integers: \u001b[1m[\u001b[0m\u001b[1;36m15496\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m466\u001b[0m, \u001b[1;36m345\u001b[0m, \u001b[1;36m588\u001b[0m, \u001b[1;36m8887\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m220\u001b[0m, \u001b[1;36m50256\u001b[0m, \u001b[1;36m554\u001b[0m, \u001b[1;36m262\u001b[0m, \u001b[1;36m4252\u001b[0m, \u001b[1;36m18250\u001b[0m, \u001b[1;36m8812\u001b[0m, \u001b[1;36m2114\u001b[0m, \u001b[1;36m286\u001b[0m, \u001b[1;36m617\u001b[0m, \u001b[1;36m34680\u001b[0m, \n",
       "\u001b[1;36m27271\u001b[0m, \u001b[1;36m13\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Strings: Hello, do you like tea? <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">|endoftext|</span><span style=\"font-weight: bold\">&gt;</span> In the sunlit terraces of someunknownPlace.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Strings: Hello, do you like tea? \u001b[1m<\u001b[0m\u001b[1;95m|endoftext|\u001b[0m\u001b[1m>\u001b[0m In the sunlit terraces of someunknownPlace.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Unknown token: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">|endoftext|</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Unknown token: \u001b[1m<\u001b[0m\u001b[1;95m|endoftext|\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Integers: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33901</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">86</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">343</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">86</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">220</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">959</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Integers: \u001b[1m[\u001b[0m\u001b[1;36m33901\u001b[0m, \u001b[1;36m86\u001b[0m, \u001b[1;36m343\u001b[0m, \u001b[1;36m86\u001b[0m, \u001b[1;36m220\u001b[0m, \u001b[1;36m959\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ak', 'w', 'ir', 'w', ' ', 'ier']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Strings: Akwirw ier\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Strings: Akwirw ier\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "from rich import print as rprint\n",
    "\n",
    "rprint(f\"tiktoken version: {version('tiktoken')}\")\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces \"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(\n",
    "    text, \n",
    "    allowed_special={\"<|endoftext|>\"}\n",
    ")\n",
    "\n",
    "rprint(f\"Integers: {integers}\")\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "rprint(f\"Strings: {strings}\")\n",
    "\n",
    "# Get unknown token\n",
    "unknown_token = tokenizer.decode([50256])\n",
    "rprint(f\"Unknown token: {unknown_token}\")\n",
    "\n",
    "## Byte pair encoding of unknown words Exercise 2.1\n",
    "text_new = \"Akwirw ier\"\n",
    "\n",
    "integers = tokenizer.encode(\n",
    "    text_new, allowed_special={\"<|endoftext|>\"}\n",
    ")\n",
    "rprint(f\"Integers: {integers}\")\n",
    "\n",
    "# decode each individual token ID\n",
    "print([tokenizer.decode([i]) for i in integers])\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "rprint(f\"Strings: {strings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window\n",
    "\n",
    "The next step in creating the embeddings for the LLM is to generate the input‚Äìtarget pairs required for training an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The total number of tokens in the training set, after applying the BPE tokenizer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5145</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The total number of tokens in the training set, after applying the BPE tokenizer: \u001b[1;36m5145\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Executing this code will return 5145,\n",
       "    the total number of tokens in the training set,\n",
       "    after applying the BPE tokenizer."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4920</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m290\u001b[0m, \u001b[1;36m4920\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "One of the easiest and most intuitive ways to create \n",
       "    the input-target pairs for the next-word prediction task \n",
       "    is to create two variables, x and y, where x contains \n",
       "    the input tokens and y contains the targets, \n",
       "    which are the inputs shifted by 1:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4920</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2241</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">287</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "features: \u001b[1m[\u001b[0m\u001b[1;36m290\u001b[0m, \u001b[1;36m4920\u001b[0m, \u001b[1;36m2241\u001b[0m, \u001b[1;36m287\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">labels:          <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4920</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2241</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">287</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "labels:          \u001b[1m[\u001b[0m\u001b[1;36m4920\u001b[0m, \u001b[1;36m2241\u001b[0m, \u001b[1;36m287\u001b[0m, \u001b[1;36m257\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">features <span style=\"font-weight: bold\">(</span>decoded<span style=\"font-weight: bold\">)</span>:  and established himself in\n",
       "</pre>\n"
      ],
      "text/plain": [
       "features \u001b[1m(\u001b[0mdecoded\u001b[1m)\u001b[0m:  and established himself in\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">labels <span style=\"font-weight: bold\">(</span>decoded<span style=\"font-weight: bold\">)</span>:         established himself in a\n",
       "</pre>\n"
      ],
      "text/plain": [
       "labels \u001b[1m(\u001b[0mdecoded\u001b[1m)\u001b[0m:         established himself in a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "By processing the inputs along with the targets, \n",
       "    which are the inputs shifted by one position, \n",
       "    we can create the next-word prediction tasks"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span><span style=\"font-weight: bold\">]</span>\n",
       "----&gt; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4920</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m290\u001b[0m\u001b[1m]\u001b[0m\n",
       "----> \u001b[1;36m4920\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4920</span><span style=\"font-weight: bold\">]</span>\n",
       "----&gt; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2241</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m290\u001b[0m, \u001b[1;36m4920\u001b[0m\u001b[1m]\u001b[0m\n",
       "----> \u001b[1;36m2241\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4920</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2241</span><span style=\"font-weight: bold\">]</span>\n",
       "----&gt; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">287</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m290\u001b[0m, \u001b[1;36m4920\u001b[0m, \u001b[1;36m2241\u001b[0m\u001b[1m]\u001b[0m\n",
       "----> \u001b[1;36m287\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4920</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2241</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">287</span><span style=\"font-weight: bold\">]</span>\n",
       "----&gt; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m290\u001b[0m, \u001b[1;36m4920\u001b[0m, \u001b[1;36m2241\u001b[0m, \u001b[1;36m287\u001b[0m\u001b[1m]\u001b[0m\n",
       "----> \u001b[1;36m257\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> and ----&gt;  established\n",
       "</pre>\n"
      ],
      "text/plain": [
       " and ---->  established\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> and established ----&gt;  himself\n",
       "</pre>\n"
      ],
      "text/plain": [
       " and established ---->  himself\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> and established himself ----&gt;  in\n",
       "</pre>\n"
      ],
      "text/plain": [
       " and established himself ---->  in\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> and established himself in ----&gt;  a\n",
       "</pre>\n"
      ],
      "text/plain": [
       " and established himself in ---->  a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "import tiktoken\n",
    "from rich import print as rprint\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "text_str = 'The total number of tokens in the training set, after applying the BPE tokenizer'\n",
    "rprint(f\"{text_str}: {len(enc_text)}\")\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"Executing this code will return 5145,\n",
    "    the total number of tokens in the training set,\n",
    "    after applying the BPE tokenizer.\"\"\"\n",
    "))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "rprint(f\"{enc_sample[:2]}\")\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"One of the easiest and most intuitive ways to create \n",
    "    the input-target pairs for the next-word prediction task \n",
    "    is to create two variables, x and y, where x contains \n",
    "    the input tokens and y contains the targets, \n",
    "    which are the inputs shifted by 1:\"\"\"\n",
    "))\n",
    "\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1 : context_size + 1]  # Shifted by 1\n",
    "rprint(f\"features: {x}\")\n",
    "rprint(f\"labels: \\t {y}\")\n",
    "rprint(f\"features (decoded): {tokenizer.decode(x)}\")\n",
    "rprint(f\"labels (decoded): \\t {tokenizer.decode(y)}\")\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"By processing the inputs along with the targets, \n",
    "    which are the inputs shifted by one position, \n",
    "    we can create the next-word prediction tasks\"\"\"\n",
    "))\n",
    "\n",
    "# Setting up for next word prediction\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    rprint(context, \"---->\", desired)\n",
    "\n",
    "# # Decoding\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    rprint(\n",
    "        tokenizer.decode(context),\n",
    "        \"---->\",\n",
    "        tokenizer.decode([desired]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We've now created the input‚Äìtarget pairs that we can use for LLM training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "\n",
    "There's only one more task before we can turn the tokens into embeddings: implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays. In particular, we are interested in returning two tensors:\n",
    "* An input tensor containing the text that the LLM sees;\n",
    "* A target tensor that includes the targets for the LLM to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Output first batch (stride=1):"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Inputs:\n",
       " <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">367</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2885</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1464</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Inputs:\n",
       " \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m  \u001b[1;36m40\u001b[0m,  \u001b[1;36m367\u001b[0m, \u001b[1;36m2885\u001b[0m, \u001b[1;36m1464\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Targets:\n",
       " <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">367</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2885</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1464</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1807</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Targets:\n",
       " \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m367\u001b[0m, \u001b[1;36m2885\u001b[0m, \u001b[1;36m1464\u001b[0m, \u001b[1;36m1807\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Output second batch (stride=1):"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">367</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2885</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1464</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1807</span><span style=\"font-weight: bold\">]])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2885</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1464</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1807</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3619</span><span style=\"font-weight: bold\">]])]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m367\u001b[0m, \u001b[1;36m2885\u001b[0m, \u001b[1;36m1464\u001b[0m, \u001b[1;36m1807\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2885\u001b[0m, \u001b[1;36m1464\u001b[0m, \u001b[1;36m1807\u001b[0m, \u001b[1;36m3619\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "A stride of 4 moves the input field by 4 positions:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Inputs:\n",
       " <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">367</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2885</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1464</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1807</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3619</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">402</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">271</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10899</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2138</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7026</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15632</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">438</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">922</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5891</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1576</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">438</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">568</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">340</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">373</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">645</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1049</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5975</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">502</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3285</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Inputs:\n",
       " \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m   \u001b[1;36m40\u001b[0m,   \u001b[1;36m367\u001b[0m,  \u001b[1;36m2885\u001b[0m,  \u001b[1;36m1464\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m1807\u001b[0m,  \u001b[1;36m3619\u001b[0m,   \u001b[1;36m402\u001b[0m,   \u001b[1;36m271\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m10899\u001b[0m,  \u001b[1;36m2138\u001b[0m,   \u001b[1;36m257\u001b[0m,  \u001b[1;36m7026\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m15632\u001b[0m,   \u001b[1;36m438\u001b[0m,  \u001b[1;36m2016\u001b[0m,   \u001b[1;36m257\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m922\u001b[0m,  \u001b[1;36m5891\u001b[0m,  \u001b[1;36m1576\u001b[0m,   \u001b[1;36m438\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m568\u001b[0m,   \u001b[1;36m340\u001b[0m,   \u001b[1;36m373\u001b[0m,   \u001b[1;36m645\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m1049\u001b[0m,  \u001b[1;36m5975\u001b[0m,   \u001b[1;36m284\u001b[0m,   \u001b[1;36m502\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m284\u001b[0m,  \u001b[1;36m3285\u001b[0m,   \u001b[1;36m326\u001b[0m,    \u001b[1;36m11\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Targets:\n",
       " <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">367</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2885</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1464</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1807</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3619</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">402</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">271</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10899</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2138</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7026</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15632</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">438</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">922</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5891</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1576</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">438</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">568</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">340</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">373</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">645</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1049</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5975</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">502</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3285</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">287</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Targets:\n",
       " \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m  \u001b[1;36m367\u001b[0m,  \u001b[1;36m2885\u001b[0m,  \u001b[1;36m1464\u001b[0m,  \u001b[1;36m1807\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m3619\u001b[0m,   \u001b[1;36m402\u001b[0m,   \u001b[1;36m271\u001b[0m, \u001b[1;36m10899\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m2138\u001b[0m,   \u001b[1;36m257\u001b[0m,  \u001b[1;36m7026\u001b[0m, \u001b[1;36m15632\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m438\u001b[0m,  \u001b[1;36m2016\u001b[0m,   \u001b[1;36m257\u001b[0m,   \u001b[1;36m922\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m5891\u001b[0m,  \u001b[1;36m1576\u001b[0m,   \u001b[1;36m438\u001b[0m,   \u001b[1;36m568\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m340\u001b[0m,   \u001b[1;36m373\u001b[0m,   \u001b[1;36m645\u001b[0m,  \u001b[1;36m1049\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m5975\u001b[0m,   \u001b[1;36m284\u001b[0m,   \u001b[1;36m502\u001b[0m,   \u001b[1;36m284\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m3285\u001b[0m,   \u001b[1;36m326\u001b[0m,    \u001b[1;36m11\u001b[0m,   \u001b[1;36m287\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from rich import print as rprint\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(\n",
    "            0, len(token_ids) - max_length, stride\n",
    "        ):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[\n",
    "                i + 1 : i + max_length + 1\n",
    "            ]\n",
    "            # target_chunk = token_ids[\n",
    "            #     i + stride : i + max_length + stride\n",
    "            # ]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(\n",
    "                torch.tensor(target_chunk)\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(\n",
    "    txt,\n",
    "    batch_size=4,\n",
    "    max_length=256,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(\n",
    "        txt, tokenizer, max_length, stride\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=1,\n",
    "    max_length=4,\n",
    "    stride=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"Output first batch (stride=1):\"\"\"\n",
    "))\n",
    "\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "rprint(f\"Inputs:\\n {inputs}\")\n",
    "rprint(f\"\\nTargets:\\n {targets}\")\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"Output second batch (stride=1):\"\"\"\n",
    "))\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "rprint(second_batch)\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"A stride of 4 moves the input field by 4 positions:\"\"\"\n",
    "))\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "rprint(\"Inputs:\\n\", inputs)\n",
    "rprint(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create token embeddings\n",
    "\n",
    "The last step in preparing the input text for LLM training is to convert the token IDs into embedding vectors.\n",
    "\n",
    "**Why do we need embeddings?**\n",
    "\n",
    "A continuous vector representation, or embedding, is necessary since GPT-like LLMs are deep neural networks trained with the backpropagation algorithm.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/3.png\" width=\"450px\">\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/5.png\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Parameter containing:\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3374</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1778</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1690</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9178</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5810</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3010</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2753</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2010</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1606</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4015</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9666</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1481</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1589</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3255</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6315</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.8400</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7849</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4096</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">requires_grad</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Parameter containing:\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.3374\u001b[0m, \u001b[1;36m-0.1778\u001b[0m, \u001b[1;36m-0.1690\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m0.9178\u001b[0m,  \u001b[1;36m1.5810\u001b[0m,  \u001b[1;36m1.3010\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m1.2753\u001b[0m, \u001b[1;36m-0.2010\u001b[0m, \u001b[1;36m-0.1606\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-0.4015\u001b[0m,  \u001b[1;36m0.9666\u001b[0m, \u001b[1;36m-1.1481\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-1.1589\u001b[0m,  \u001b[1;36m0.3255\u001b[0m, \u001b[1;36m-0.6315\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-2.8400\u001b[0m, \u001b[1;36m-0.7849\u001b[0m, \u001b[1;36m-1.4096\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mrequires_grad\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Now, let's apply it to a token ID to obtain the embedding vector\n",
       "    (Python starts with a zero index, so it's the row corresponding to index 3):\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4015</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9666</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1481</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">SelectBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.4015\u001b[0m,  \u001b[1;36m0.9666\u001b[0m, \u001b[1;36m-1.1481\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mSelectBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4015</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9666</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1481</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">EmbeddingBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.4015\u001b[0m,  \u001b[1;36m0.9666\u001b[0m, \u001b[1;36m-1.1481\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mEmbeddingBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(123)\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "rprint(embedding_layer.embedding_dim)\n",
    "rprint(embedding_layer.weight)\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"Now, let's apply it to a token ID to obtain the embedding vector\n",
    "    (Python starts with a zero index, so it's the row corresponding to index 3):\n",
    "    \"\"\"\n",
    "))\n",
    "\n",
    "rprint(embedding_layer.weight[3])\n",
    "rprint(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3374</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9178</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2753</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4015</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1589</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.8400</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1778</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5810</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2010</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9666</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3255</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7849</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1690</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3010</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1606</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1481</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6315</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4096</span><span style=\"font-weight: bold\">]]</span>,\n",
       "       <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">PermuteBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.3374\u001b[0m,  \u001b[1;36m0.9178\u001b[0m,  \u001b[1;36m1.2753\u001b[0m, \u001b[1;36m-0.4015\u001b[0m, \u001b[1;36m-1.1589\u001b[0m, \u001b[1;36m-2.8400\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-0.1778\u001b[0m,  \u001b[1;36m1.5810\u001b[0m, \u001b[1;36m-0.2010\u001b[0m,  \u001b[1;36m0.9666\u001b[0m,  \u001b[1;36m0.3255\u001b[0m, \u001b[1;36m-0.7849\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-0.1690\u001b[0m,  \u001b[1;36m1.3010\u001b[0m, \u001b[1;36m-0.1606\u001b[0m, \u001b[1;36m-1.1481\u001b[0m, \u001b[1;36m-0.6315\u001b[0m, \u001b[1;36m-1.4096\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mPermuteBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2753</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4015</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.8400</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9178</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2010</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9666</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7849</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5810</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1606</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1481</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4096</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3010</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">PermuteBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.2753\u001b[0m, \u001b[1;36m-0.4015\u001b[0m, \u001b[1;36m-2.8400\u001b[0m,  \u001b[1;36m0.9178\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-0.2010\u001b[0m,  \u001b[1;36m0.9666\u001b[0m, \u001b[1;36m-0.7849\u001b[0m,  \u001b[1;36m1.5810\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m-0.1606\u001b[0m, \u001b[1;36m-1.1481\u001b[0m, \u001b[1;36m-1.4096\u001b[0m,  \u001b[1;36m1.3010\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mPermuteBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Direct look-up vs one-hot encoding\n",
    "torch.manual_seed(123)\n",
    "\n",
    "num_idx = max(input_ids)\n",
    "rprint(input_ids)\n",
    "\n",
    "# From notebook example\n",
    "onehot = torch.nn.functional.one_hot(input_ids)\n",
    "rprint(onehot)\n",
    "\n",
    "linear = torch.nn.Linear(num_idx, output_dim, bias=False)\n",
    "rprint(embedding_layer.weight.T)\n",
    "\n",
    "linear.weight = torch.nn.Parameter(embedding_layer.weight.T.detach())\n",
    "rprint(linear(onehot.float()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Having now created embedding vectors from token IDs, next we‚Äôll add a small modification to these embedding vectors to encode positional information about a token within a text.\n",
    "\n",
    "To achieve this, we can use two broad categories of position-aware embeddings:\n",
    "\n",
    "* Relative positional embeddings, *\"How far apart?\"*\n",
    "* Absolute positional embeddings, *\"At which exact position?\"* (OpenAI).\n",
    "\n",
    "Both types of positional embeddings aim to augment the capacity of LLMs to understand the order and relationships between tokens, ensuring more **accurate** and **context-aware** predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The token ID tensor is 8 x 4 dimensional, meaning that the data batch \n",
       "    consists of eight text samples with four tokens each."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Token IDs:\n",
       " <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">367</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2885</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1464</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1807</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3619</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">402</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">271</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10899</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2138</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7026</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15632</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">438</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">922</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5891</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1576</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">438</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">568</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">340</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">373</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">645</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1049</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5975</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">502</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">284</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3285</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Token IDs:\n",
       " \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m   \u001b[1;36m40\u001b[0m,   \u001b[1;36m367\u001b[0m,  \u001b[1;36m2885\u001b[0m,  \u001b[1;36m1464\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m1807\u001b[0m,  \u001b[1;36m3619\u001b[0m,   \u001b[1;36m402\u001b[0m,   \u001b[1;36m271\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m10899\u001b[0m,  \u001b[1;36m2138\u001b[0m,   \u001b[1;36m257\u001b[0m,  \u001b[1;36m7026\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m15632\u001b[0m,   \u001b[1;36m438\u001b[0m,  \u001b[1;36m2016\u001b[0m,   \u001b[1;36m257\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m922\u001b[0m,  \u001b[1;36m5891\u001b[0m,  \u001b[1;36m1576\u001b[0m,   \u001b[1;36m438\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m568\u001b[0m,   \u001b[1;36m340\u001b[0m,   \u001b[1;36m373\u001b[0m,   \u001b[1;36m645\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m1049\u001b[0m,  \u001b[1;36m5975\u001b[0m,   \u001b[1;36m284\u001b[0m,   \u001b[1;36m502\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m  \u001b[1;36m284\u001b[0m,  \u001b[1;36m3285\u001b[0m,   \u001b[1;36m326\u001b[0m,    \u001b[1;36m11\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Inputs shape:\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Inputs shape:\n",
       "\n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "If we have a batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor.\n",
       "    Let's now use the embedding layer to embed these token IDs into 256-dimensional vectors:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Converting to 8x4x256 dimensional tensor output:\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Converting to 8x4x256 dimensional tensor output:\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Add absolute positional embeddings:\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Add absolute positional embeddings:\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Max context length: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Max context length: \u001b[1;36m4\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "PyTorch will add the 4x256-dimensional <code>pos_embeddings</code> tensor to each\n",
       "    4x256-dimensional token embedding tensor in each of the 8 batches."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Adding four <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>-dimensional vectors to token embeddings:\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Adding four \u001b[1;36m256\u001b[0m-dimensional vectors to token embeddings:\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Each sample in the batch consists of a tensor of shape (4, 256), \n",
       "    representing the embeddings for 4 tokens."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Each sample in the batch consists of a tensor of shape <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">)</span>:\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Each sample in the batch consists of a tensor of shape \u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m)\u001b[0m:\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.9433</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2660</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3694</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0554</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.8439</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3893</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.3940</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.1606</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.6590</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.2740</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2962</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5062</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6351</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7310</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.1727</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.8074</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8498</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0326</span><span style=\"font-weight: bold\">]</span>,\n",
       "         <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5251</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.9440</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.6766</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0733</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3402</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0870</span><span style=\"font-weight: bold\">]]]</span>,\n",
       "       <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">SliceBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.9433\u001b[0m,  \u001b[1;36m0.2660\u001b[0m, \u001b[1;36m-0.3694\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.0554\u001b[0m,  \u001b[1;36m1.8439\u001b[0m, \u001b[1;36m-0.3893\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m2.3940\u001b[0m,  \u001b[1;36m2.1606\u001b[0m, \u001b[1;36m-2.6590\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m2.2740\u001b[0m, \u001b[1;36m-0.2962\u001b[0m, \u001b[1;36m-1.5062\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m0.6351\u001b[0m,  \u001b[1;36m0.7310\u001b[0m,  \u001b[1;36m2.1727\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m2.8074\u001b[0m, \u001b[1;36m-0.8498\u001b[0m, \u001b[1;36m-1.0326\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m \u001b[1;36m1.5251\u001b[0m, \u001b[1;36m-0.9440\u001b[0m, \u001b[1;36m-2.6766\u001b[0m,  \u001b[33m...\u001b[0m,  \u001b[1;36m0.0733\u001b[0m, \u001b[1;36m-2.3402\u001b[0m, \u001b[1;36m-0.0870\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mSliceBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Define vocab and embedding dimensions\n",
    "vocab_size = 50257  # GPT-3\n",
    "output_dim = 256\n",
    "\n",
    "# Initializes with random weights\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"The token ID tensor is 8 x 4 dimensional, meaning that the data batch \n",
    "    consists of eight text samples with four tokens each.\"\"\"\n",
    "))\n",
    "\n",
    "console.print(\"Token IDs:\\n\", inputs)\n",
    "console.print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"If we have a batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor.\n",
    "    Let's now use the embedding layer to embed these token IDs into 256-dimensional vectors:\"\"\"\n",
    "))\n",
    "\n",
    "# # Convert to 8x4x256 dimensional tensor output\n",
    "console.print('Converting to 8x4x256 dimensional tensor output:\\n')\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "console.print(token_embeddings.shape)\n",
    "\n",
    "# add absolute embedding approach\n",
    "console.print('\\nAdd absolute positional embeddings:\\n')\n",
    "context_length = max_length\n",
    "console.print(f\"Max context length: {max_length}\")\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "console.print(pos_embeddings.shape)\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"PyTorch will add the 4x256-dimensional <code>pos_embeddings</code> tensor to each\n",
    "    4x256-dimensional token embedding tensor in each of the 8 batches.\"\"\"\n",
    "))\n",
    "\n",
    "# # Adding four 256-dimensional vectors to token embeddings\n",
    "console.print('\\nAdding four 256-dimensional vectors to token embeddings:\\n')\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "console.print(input_embeddings.shape)\n",
    "\n",
    "display(HTML(\n",
    "    \"\"\"Each sample in the batch consists of a tensor of shape (4, 256), \n",
    "    representing the embeddings for 4 tokens.\"\"\"\n",
    "))\n",
    "\n",
    "console.print('\\nEach sample in the batch consists of a tensor of shape (4, 256):\\n')\n",
    "console.print(input_embeddings[3:4])\n",
    "console.print(input_embeddings[3:4].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-project-nVfOrddR-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
